{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import matplotlib \n",
    "import matplotlib.style\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import re\n",
    "import math\n",
    "from matplotlib.collections import LineCollection\n",
    "import time\n",
    "import itertools\n",
    "\n",
    "\n",
    "from tkinter import *\n",
    "from tkinter import messagebox\n",
    "import tkinter as tk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Auxiliary Classes: SimpleTableInput and Framegeneration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Support Class to Frame Generation\n",
    "class SimpleTableInput(tk.Frame):\n",
    "    def __init__(self, parent, rows, columns):\n",
    "        tk.Frame.__init__(self, parent)\n",
    "\n",
    "        self._entry = {}\n",
    "        self.rows = rows\n",
    "        self.columns = columns\n",
    "\n",
    "        # Real Time Validation Command\n",
    "        vcmd = (self.register(self._validate), \"%P\")\n",
    "\n",
    "        # Create a table of TkInter Entries with the use of Grid.\n",
    "        for row in range(self.rows):\n",
    "            for column in range(self.columns):\n",
    "                index = (row, column)\n",
    "                e = tk.Entry(self, validate=\"key\", validatecommand=vcmd)\n",
    "                e.grid(row=row, column=column, stick=\"nsew\")\n",
    "                self._entry[index] = e\n",
    "        # Column alignement\n",
    "        for column in range(self.columns):\n",
    "            self.grid_columnconfigure(column, weight=1)\n",
    "        #Filling extra space\n",
    "        self.grid_rowconfigure(rows, weight=1)\n",
    "\n",
    "        # Labels\n",
    "        L1 = Label(parent, text=\"Introduce a valid board with no repeated indexes\")\n",
    "        L1.pack()\n",
    "        L2 = Label(parent, text=\"Only values from 0 to \" + str((self.rows*self.columns)-1) + \" are allowed for this matrix\")\n",
    "        L2.pack()\n",
    "        \n",
    "\n",
    "    def get(self):\n",
    "        #Fetches the entries data and store it on a list of arrays\n",
    "        result = []\n",
    "        for row in range(self.rows):\n",
    "            current_row = []\n",
    "            for column in range(self.columns):\n",
    "                index = (row, column)\n",
    "                #Check if a empty value was introduced in the matrix\n",
    "                if self._entry[index].get() == \"\":\n",
    "                \treturn False\n",
    "                current_row.append(int(self._entry[index].get()))\n",
    "            result.append(current_row)\n",
    "            \n",
    "        #Repeated Numbers on the Matrix Check\n",
    "        resultnum = []\n",
    "        for i in range(len(result)):\n",
    "            for j in range(len(result[i])):\n",
    "                resultnum.append(result[i][j])\n",
    "        uniques = np.unique(resultnum, axis = 0)\n",
    "        if len(uniques) != self.rows * self.columns:\n",
    "        \treturn False\n",
    "        #Return Result\n",
    "        return result\n",
    "    \n",
    "    def _validate(self, P):\n",
    "      #Performs Input Validation on real time\n",
    "\n",
    "        if P.strip() == \"\":\n",
    "        \treturn True\n",
    "\n",
    "        try:\n",
    "            f = int(P)\n",
    "            #Doesn't allows to introduce indexes over rows*columns\n",
    "            if f > (self.rows * self.columns) -1:\n",
    "            \tself.bell()\n",
    "            \treturn False\n",
    "            #Only allows for Integers\n",
    "        except ValueError:\n",
    "            self.bell()\n",
    "            return False\n",
    "\n",
    "        return True\n",
    "#Main Class in charge of Matrix Frame Generation for TkInter\n",
    "class Framegeneration(tk.Frame):\n",
    "    #Creates a frame for TkInter\n",
    "    def __init__(self, parent,states, actions):\n",
    "        tk.Frame.__init__(self, parent)\n",
    "        self.table = SimpleTableInput(self, states, actions)\n",
    "        self.submit = tk.Button(self, text=\"Submit\", command=lambda: self.on_submit(parent))\n",
    "        self.table.pack(side=\"top\", fill=\"both\", expand=True)\n",
    "        self.submit.pack(side=\"bottom\")\n",
    "    #Controls the submit button on the tkinter frame\n",
    "    def on_submit(self, parent):\n",
    "        #Check if the matrix was properly introduced\n",
    "        if self.table.get() == False:\n",
    "            Qlearn.errorWindow(self)\n",
    "            return False\n",
    "        Qlearn.pargui(self.table.get(), parent)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Class Declaration Q-Learn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "BackEnd Auxiliary Methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Main Class\n",
    "class Qlearn():\n",
    "\n",
    "    #BackEnd Auxiliary methods\n",
    "\n",
    "    #Turns a String Array Into a Integer Array\n",
    "    def stringToArrayInt(checkpoint,win2):\n",
    "        res = []\n",
    "        array = checkpoint.strip().split(\",\")\n",
    "        for j in range(len(array)):\n",
    "            if int(array[j]) > (Qlearn.states * Qlearn.actions)-1:\n",
    "                Qlearn.errorWindow(win2)\n",
    "                return False\n",
    "            res.append(int(array[j]))\n",
    "        return res\n",
    "    #Eliminates specified restrictions from the neighbors list\n",
    "    def applyRestrictions(neighbors, restrictions,win2):\n",
    "        array = restrictions.strip().split(\";\")\n",
    "        for i in range(len(array)):\n",
    "            current = array[i].strip().split(\"-\")\n",
    "            if int(current[0]) > (Qlearn.states * Qlearn.actions)-1 or int(current[1]) > (Qlearn.states * Qlearn.actions)-1:\n",
    "                Qlearn.errorWindow(win2)\n",
    "                return False\n",
    "            for j in range(len(neighbors)):\n",
    "                if neighbors[j][0] == int(current[0]):\n",
    "                    ind = neighbors[j][1].index(int(current[1]))\n",
    "                    neighbors[j][1].pop(ind)\n",
    "                elif neighbors[j][0] == int(current[1]):\n",
    "                    ind = neighbors[j][1].index(int(current[0]))\n",
    "                    neighbors[j][1].pop(ind)\n",
    "        return neighbors\n",
    "\n",
    "    #This method generates a tuple graph with the possible movements of every state on the matrix\n",
    "    #Note that the state itself is not reachable from itself. Only the goal square has that property, and it's not controlled in this method.\n",
    "    def find_neighbours(arr):\n",
    "\n",
    "        final_neighbors = []\n",
    "\n",
    "        for i in range(len(arr)):\n",
    "            for j, value in enumerate(arr[i]):\n",
    "                if i == 0 or i == len(arr) - 1 or j == 0 or j == len(arr[i]) - 1:\n",
    "                    # When arr[i][j] object is not a center square\n",
    "                    # We must check which neighbours are present before evaluating them\n",
    "                    neighbors = []\n",
    "                    if i != 0:\n",
    "                        neighbors.append(arr[i - 1][j])  #Top\n",
    "                    if j != len(arr[i]) - 1:\n",
    "                        neighbors.append(arr[i][j + 1])  #Right\n",
    "                    if i != len(arr) - 1:\n",
    "                        neighbors.append(arr[i + 1][j])  #Bottom\n",
    "                    if j != 0:\n",
    "                        neighbors.append(arr[i][j - 1])  #Left\n",
    "                    if i!= 0 and j != len(arr[i]) -1:\n",
    "                        neighbors.append(arr[i-1][j+1])  #Top Right\n",
    "                    if i!= 0 and j!= 0:\n",
    "                        neighbors.append(arr[i-1][j-1])  #Top Left\n",
    "                    if i!= len(arr) - 1 and j != len(arr[i]) - 1:\n",
    "                        neighbors.append(arr[i+1][j+1])  #Bottom Right\n",
    "                    if i!= len(arr) -1 and j!= 0:\n",
    "                        neighbors.append(arr[i+1][j-1])  #Bottom Left\n",
    "                        \n",
    "\n",
    "                else:\n",
    "                    #Center Squares\n",
    "                    neighbors = [\n",
    "                        arr[i - 1][j],  # Top\n",
    "                        arr[i][j + 1],  # Right\n",
    "                        arr[i + 1][j],  # Bottom\n",
    "                        arr[i][j - 1],  # Left\n",
    "                        arr[i-1][j-1],  # Top Left\n",
    "                        arr[i-1][j+1],  # Top Right\n",
    "                        arr[i+1][j-1],  # Bottom Left\n",
    "                        arr[i+1][j+1]   # Bottom Right  \n",
    "                    ]\n",
    "                final_neighbors.append([value, neighbors])\n",
    "        return final_neighbors\n",
    "\n",
    "    def rendimiento(q, states, actions):\n",
    "        suma = 0\n",
    "        biggest = 0\n",
    "        #This loop gets the biggest number on the Q matrix\n",
    "        for i in range(states):\n",
    "            for j in range(actions):\n",
    "                suma += q[i][j]\n",
    "                if biggest < q[i][j]:\n",
    "                    biggest = q[i][j]\n",
    "        #Calculation of the performance measure after fetching the biggest number on the Q matrix           \n",
    "        ren = (suma/biggest)*100\n",
    "        return ren          \n",
    "\n",
    "\n",
    "    def maximum(foresight, next_state, q):\n",
    "        result = -1\n",
    "        #This loop fetches the maximum q value for the next foresighted actions possible\n",
    "        for i in range(len(foresight)):\n",
    "            if q[next_state][foresight[i]] > result:\n",
    "                result = q[next_state][foresight[i]]\n",
    "            \n",
    "        return result\n",
    "\n",
    "    def getAction(current, states, actions,r):\n",
    "        arr = []\n",
    "        #This loop collects all the possible actions, when they get a reward of 0 or a reward of 100, discards all -1 (-100 in Phase3) rewards\n",
    "        for i in range(states):\n",
    "            if r[(current,i)] == 0 or r[(current,i)] == -50 or r[(current,i)] == 10:\n",
    "                arr.append(i)\n",
    "            if r[(current,i)] == 100:\n",
    "                arr.append(i)\n",
    "        return arr\n",
    "\n",
    "    def normalize(q, states, actions):\n",
    "        tuvi = np.max(q)\n",
    "        q = q*(100/tuvi)\n",
    "        for i in range(states):\n",
    "            for j in range(actions):\n",
    "                q[i][j] = math.ceil(q[i][j])\n",
    "        return q\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "FrontEnd Auxiliary Methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Qlearn(Qlearn):\n",
    "#FrontEnd Auxiliary Methods\n",
    "\n",
    "        #Checks if the fields are not empty\n",
    "    def passToFase1(Eep, Ego,Ega,Eis,win2):\n",
    "        try:\n",
    "            episodes = int(Eep.get())\n",
    "            goal = int(Ego.get())\n",
    "            gamma = float(Ega.get())\n",
    "            initial = int(Eis)\n",
    "            if goal > (Qlearn.states * Qlearn.actions)-1 or initial > (Qlearn.states * Qlearn.actions)-1:\n",
    "                return Qlearn.errorWindow(win2)\n",
    "        except ValueError:\n",
    "            return Qlearn.errorWindow(win2)\n",
    "\n",
    "        return Qlearn.fase1(Eep, Ego,Ega,Eis,win2)\n",
    "        #Check if the fields have the correct kind of value and are not empty\n",
    "    def passToFase2(Eep, Ego,Ega,Eeps, Eaph,Eis,win2):\n",
    "        try:\n",
    "            episodes = int(Eep)\n",
    "            goal = int(Ego)\n",
    "            gamma = float(Ega)\n",
    "            initial = int(Eis)\n",
    "            alpha = float(Eaph)\n",
    "            Epsilon = float(Eeps)\n",
    "            if goal > (Qlearn.states * Qlearn.actions)-1 or initial > (Qlearn.states * Qlearn.actions)-1:\n",
    "                return Qlearn.errorWindow(win2)\n",
    "        except ValueError:\n",
    "            return Qlearn.errorWindow(win2)\n",
    "\n",
    "        return Qlearn.fase2(Eep, Ego,Ega,Eeps,Eaph,Eis,win2)\n",
    "        \n",
    "        #Check that the fields are not empty or the strings introduced matches the required format to process\n",
    "    def passToFase3(Eep, Ego,Ega,Eis,Ech,Ere,Ebo,win2):\n",
    "        correctValues = True\n",
    "        try:\n",
    "            episodes = int(Eep)\n",
    "            goal = int(Ego)\n",
    "            gamma = float(Ega)\n",
    "            initial = int(Eis)\n",
    "            if goal > (Qlearn.states * Qlearn.actions)-1 or initial > (Qlearn.states * Qlearn.actions)-1:\n",
    "                return Qlearn.errorWindow(win2)\n",
    "        except ValueError:\n",
    "            correctValues = False\n",
    "\n",
    "        patternRes = re.compile(\"^((\\d+-\\d+;)+\\d+-\\d+|(\\d+-\\d+){1}){1}$|^$\")\n",
    "        patternArr = re.compile(\"^(\\d+,)+(\\d+){1}|(\\d+){1}$|^$\")\n",
    "\n",
    "        if not re.match(patternRes, Ere) or not re.match(patternArr,Ech) or not re.match(patternArr,Ebo) or not correctValues:\n",
    "            return Qlearn.errorWindow(win2)\n",
    "\n",
    "\n",
    "        return Qlearn.fase3(Eep, Ego,Ega,Eis,Ech,Ere,Ebo,win2)\n",
    "\n",
    "    #Check that the fields are not empty or the strings introduced matches the required format to process\n",
    "    def passToFase32(Eep, Ego,Ega,Eis,Ech,Ere,Ebo,Eaph, Eeps,win2):\n",
    "        correctValues = True\n",
    "        try:\n",
    "            episodes = int(Eep)\n",
    "            goal = int(Ego)\n",
    "            gamma = float(Ega)\n",
    "            initial = int(Eis)\n",
    "            alpha = float(Eaph)\n",
    "            Epsilon = float(Eeps)\n",
    "            if goal > (Qlearn.states * Qlearn.actions)-1 or initial > (Qlearn.states * Qlearn.actions)-1:\n",
    "                return Qlearn.errorWindow(win2)\n",
    "        except ValueError:\n",
    "            correctValues = False\n",
    "\n",
    "        patternRes = re.compile(\"^((\\d+-\\d+;)+\\d+-\\d+|(\\d+-\\d+){1}){1}$|^$\")\n",
    "        patternArr = re.compile(\"^(\\d+,)+(\\d+){1}|(\\d+){1}$|^$\")\n",
    "\n",
    "        if not re.match(patternRes, Ere) or not re.match(patternArr,Ech) or not re.match(patternArr,Ebo) or not correctValues:\n",
    "            return Qlearn.errorWindow(win2)\n",
    "\n",
    "        return Qlearn.fase32(Eep, Ego,Ega,Eis,Ech,Ere,Ebo,Eaph,Eeps,win2)\n",
    "\n",
    "        #This method validates if the entry is int\n",
    "    def validateint(P):\n",
    "        if P.strip() == \"\":\n",
    "            return True\n",
    "        try:\n",
    "            f = int(P)\n",
    "        except ValueError:\n",
    "            return False\n",
    "        return True\n",
    "    #This method validates if the entry is float\n",
    "    def validatefloat(P):\n",
    "        if P.strip() == \"\":\n",
    "            return True\n",
    "        try:\n",
    "            f = float(P)\n",
    "            if f < float(0) or f > float(1):\n",
    "                return False\n",
    "        except ValueError:\n",
    "            return False\n",
    "        return True\n",
    "    \n",
    "    def render(q, states, actions):\n",
    "        #The loops fetches q_size to generate a string representation of the result matrix, using integers for the matrix values\n",
    "        map = \"+\t\t\"\n",
    "        for i in range(states):\n",
    "            map +=  str(i) + \"\t\"\n",
    "        map += '\\n' + \"-----------------------------------------------------------------------------------------------------------\"\n",
    "        for i in range(states):\n",
    "                map += '\\n' + str(i) + \"\t|\t\"\n",
    "                for j in range(actions):\n",
    "                    map += str(int(q[i][j])) + \"\t\"\n",
    "        return map\n",
    "    \n",
    "    # looks for the highest q values in the q matrix\n",
    "    def shortestpath(initialState, goal_state, q):\n",
    "        path = [initialState]\n",
    "        next_state = np.argmax(q[initialState,])\n",
    "        path.append(next_state)\n",
    "        while next_state != goal_state:\n",
    "            next_state = np.argmax(q[next_state,])\n",
    "            path.append(next_state)\n",
    "        return path\n",
    "\n",
    "\n",
    "    #Creates an error Windows in case of entry failure\n",
    "    def errorWindow(win):\n",
    "        messagebox.showerror(\"Incorrect Values Given\", \"You gave incorrect kind of values, try again next time\")\n",
    "        return print(\"Try again next time\")\n",
    "                    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reward Main Methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Qlearn(Qlearn):\n",
    "#Reward Main Methods\n",
    "\n",
    "        #This method will generate a reward matrix for any symetrical matrix\n",
    "    def reward(goal_state, states, actions, neighbors):\n",
    "        r = np.matrix(np.ones(shape=[states, actions]))\n",
    "        r = r * -1\n",
    "\n",
    "        for row in neighbors:\n",
    "            r[row] = 0\n",
    "            row = row[::-1]\n",
    "            r[row] = 0\n",
    "        \n",
    "        for i in range(states):\n",
    "            for j in range(actions):\n",
    "                #Whenever the goal state is reachable, the reward for taking that action will be 100\n",
    "                if j == goal_state and r[(i,j)] != -1:\n",
    "                    r[(i,j)] = 100\n",
    "        #When goal goes to itself, will always have 100 as reward           \n",
    "        r[(goal_state),(goal_state)] = 100\n",
    "        print(r)\n",
    "        return r\n",
    "    #Generate Reward Matrix with Checkpoint\n",
    "    def reward2(goal_state, states, actions, neighbors, boost, checkpoint):\n",
    "        r = np.matrix(np.ones(shape=[states, actions]))\n",
    "        r = r * -100\n",
    "\n",
    "        for row in neighbors:\n",
    "            r[row] = 0\n",
    "            row = row[::-1]\n",
    "            r[row] = 0\n",
    "        \n",
    "        for i in range(states):\n",
    "            for j in range(actions):\n",
    "                #Whenever the goal state is reachable, the reward for taking that action will be 100\n",
    "                if j == goal_state and r[(i,j)] != -100:\n",
    "                    r[(i,j)] = 100\n",
    "                #Pitfall Handle\n",
    "                if checkpoint != \"\":\n",
    "                    for k in range(len(checkpoint)):\n",
    "                        if j == checkpoint[k] and r[(i,j)] != -100:\n",
    "                            r[(i,checkpoint[k])] = -50\n",
    "                #Boost Handle\n",
    "                if boost != \"\":\n",
    "                    for v in range(len(boost)):\n",
    "                        if j == boost[v] and r[(i,j)] != -100:\n",
    "                            r[(i,boost[v])] = 10\n",
    "        #When goal goes to itself, will always have 100 as reward           \n",
    "        r[(goal_state),(goal_state)] = 100\n",
    "        print(r)\n",
    "        return r\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Main Algorythms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Qlearn(Qlearn):\n",
    "#Main Algorythm for Phase 1 and 2\n",
    "\n",
    "    def evaluate(q, currentState, states, actions, gamma, r):\n",
    "        #Get all possible actions for the current state\n",
    "        posactions = Qlearn.getAction(currentState, states, actions, r)\n",
    "        #Random choice of the next state among the possible ones\n",
    "        next_state = random.choice(posactions)\n",
    "        #Get all possible actions for the chosen next state\n",
    "        foresight = Qlearn.getAction(next_state, states,actions, r)\n",
    "        #Q-function that evaluates the paths\n",
    "        q[currentState][next_state] = r[(currentState,next_state)] + (gamma * Qlearn.maximum(foresight, next_state,q))\n",
    "        #Set the next_state into the currentState\n",
    "        currentState = next_state\n",
    "        return q, currentState\n",
    "    \n",
    "    def evaluateGreedy(q, currentState, states, actions, gamma, r, epsilon, alpha):\n",
    "        #espindex is a random number used for epsilon comparison\n",
    "        epsindex = random.uniform(0., 1.)\n",
    "        #Get all possible actions for the current state\n",
    "        posactions = Qlearn.getAction(currentState, states, actions, r)\n",
    "        #Epsilon is greater than random float, next_state is random among the possible options\n",
    "        next_state = random.choice(posactions)\n",
    "        \n",
    "        learn = int()\n",
    "        #Get all possible actions for the chosen next state\n",
    "        foresight = Qlearn.getAction(next_state, states, actions, r)\n",
    "        #Epsilon (Exploration Rate) is minor than random float, we add a random choice to the main equation to make the agent explore a bit\n",
    "        if epsindex < epsilon:\n",
    "            learn = q[next_state][random.choice(foresight)]\n",
    "        #On the other hand, when the other case happens we choose the best option known by our agent\n",
    "        else:\n",
    "            learn = Qlearn.maximum(foresight,next_state,q)\n",
    "        #Q-function that evaluates the paths\n",
    "        q[currentState][next_state] = r[(currentState, next_state)] + gamma * learn\n",
    "        #Set the next_state into the currentState\n",
    "        currentState = next_state\n",
    "        #At the end of each iteration, epsilon multiplies with the learning rate alpha\n",
    "        epsilon *= alpha\n",
    "        return q, currentState, epsilon\n",
    "        \n",
    "\n",
    "    #This is the main algorythm for Phase 1\n",
    "    def train(q, states, actions, episodes,initialState, goal_state, gamma, r):\n",
    "        points = []\n",
    "        start = time.clock()\n",
    "        for i in range(episodes):\n",
    "            currentState = np.random.randint(0,states-1)\n",
    "            #While we haven't reached the goal state\n",
    "            while currentState != goal_state:\n",
    "                q, currentState = Qlearn.evaluate(q, currentState, states, actions, gamma, r)\n",
    "            #Run once on the goal for the sake of convergence\n",
    "            if currentState == goal_state:\n",
    "                q, currentState = Qlearn.evaluate(q, currentState, states, actions, gamma, r)\n",
    "            rendimiento = Qlearn.rendimiento(q,states,actions)\n",
    "            point = (rendimiento)\n",
    "            points.append(point)\n",
    "            if i%5 == 0:\n",
    "                print(\"Episodio: \" + str(i))\n",
    "                print(Qlearn.render(q, states,actions))\n",
    "                print(\"Rendimiento: \" + str(rendimiento))\n",
    "        end = time.clock()\n",
    "        timetook = (end-start)*1000            \n",
    "        plt.plot(points)\n",
    "        plt.ylabel('Rendimiento')\n",
    "        plt.xlabel('Episodios')\n",
    "        plt.show()\n",
    "        print(\"El algoritmo ha tardado en ejecutar \"+str(episodes)+ \" Episodios: \" + str(timetook) + \" milisegundos\")\n",
    "        return q\n",
    "    #Main Algorythm for Phase 2\n",
    "    def train2(q, states, actions, episodes,initialState, goal_state, gamma, r, alpha, epsilon):\n",
    "        points = []\n",
    "        start = time.clock()\n",
    "        for i in range(episodes):\n",
    "            #Return the agent to the first square\n",
    "            currentState = initialState\n",
    "            #Return epsilon to the starting value\n",
    "            algepsilon = epsilon\n",
    "            while currentState != goal_state:\n",
    "                q, currentState, algepsilon = Qlearn.evaluateGreedy(q, currentState, states, actions, gamma, r, algepsilon, alpha)\n",
    "            if currentState == goal_state:\n",
    "                q, currentState, algepsilon = Qlearn.evaluateGreedy(q, currentState, states, actions, gamma, r, algepsilon, alpha)\n",
    "            rendimiento = Qlearn.rendimiento(q,states,actions)\n",
    "            point = (rendimiento)\n",
    "            points.append(point)\n",
    "            if i%5 == 0:\n",
    "                print(\"Episodio: \" + str(i))\n",
    "                print(Qlearn.render(q, states,actions))\n",
    "                print(\"Rendimiento: \" + str(Qlearn.rendimiento(q, states, actions)))\n",
    "        end = time.clock()\n",
    "        timetook = (end-start)*1000 \n",
    "        plt.plot(points)\n",
    "        plt.ylabel('Rendimiento')\n",
    "        plt.xlabel('Episodios')\n",
    "        plt.show()\n",
    "        print(\"El algoritmo ha tardado en ejecutar \"+str(episodes)+ \" Episodios: \" + str(timetook) + \" milisegundos\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Phase Handles BackEnd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Qlearn(Qlearn):\n",
    "#Phase Handles Back End\n",
    "\n",
    "    def fase1(Eep, Ego, Ega,Eis, win2):\n",
    "        #Set Initial State\n",
    "        initialState = int(Eis)\n",
    "        #Set Goal State\n",
    "        goal_state = int(Ego.get())\n",
    "        #Generate graph for creating the reward matrix\n",
    "        neighbors = Qlearn.find_neighbours(Qlearn.m)\n",
    "        #Set the Reward and Q matrix dimensions (Rows x Columns)\n",
    "        Qlearn.states = Qlearn.states * Qlearn.actions\n",
    "        Qlearn.actions = Qlearn.states\n",
    "        #Create Q Matrix with all 0\n",
    "        q = np.zeros((Qlearn.states, Qlearn.actions))\n",
    "        #Create the Reward matrix for the Entry Matrix\n",
    "        r= Qlearn.reward(goal_state, Qlearn.states, Qlearn.actions, neighbors)\n",
    "        #Set Episodes\n",
    "        episodes = int(Eep.get())\n",
    "        #Set Gamma\n",
    "        gamma = float(Ega.get())\n",
    "        #Destroy Matrix input window\n",
    "        win2.destroy()\n",
    "        #Call to Main Algorythm\n",
    "        q = Qlearn.train(q, Qlearn.states, Qlearn.actions,episodes,initialState,goal_state, gamma, r)\n",
    "        #Normalize Q output\n",
    "        q = Qlearn.normalize(q, Qlearn.states, Qlearn.actions)\n",
    "        #Print on console the result\n",
    "        print(Qlearn.render(q, Qlearn.states, Qlearn.actions))\n",
    "        #Create Result Window\n",
    "        #This is a front end window\n",
    "        res = Tk()\n",
    "        res.title(\"Q-Learning\")\n",
    "        res.geometry(\"800x400\")\n",
    "        Lres = Label(res, text = Qlearn.render(q, Qlearn.states, Qlearn.actions))\n",
    "        Lres.pack()\n",
    "        print(pd.DataFrame(q))\n",
    "        Lpath = Label(res, text = \"Camino más corto: \"+  str(Qlearn.shortestpath(initialState, goal_state, q)))\n",
    "        Lpath.pack()\n",
    "\n",
    "    def fase2(Eep, Ego, Ega, Eeps, Eaph, Eis, win2):\n",
    "        #Destroy previous Windows\n",
    "        win2.destroy()\n",
    "        #Set Initial State\n",
    "        initialState = int(Eis)\n",
    "        #Set Goal State\n",
    "        goal_state = int(Ego)\n",
    "        #Find the Neighbors to generate a graph\n",
    "        neighbors = Qlearn.find_neighbours(Qlearn.m)\n",
    "        #Set the dimensions of reward and q\n",
    "        Qlearn.states =  Qlearn.states * Qlearn.actions\n",
    "        Qlearn.actions = Qlearn.states\n",
    "        #Create the Q matrix with all 0\n",
    "        q = np.zeros((Qlearn.states, Qlearn.actions))\n",
    "        #Create the Reward Matrix for the matrix introduced in the input\n",
    "        r= Qlearn.reward(goal_state, Qlearn.states, Qlearn.actions, neighbors)\n",
    "        #Set Episodes\n",
    "        episodes = int(Eep)\n",
    "        #Set Gamma value\n",
    "        gamma = float(Ega)\n",
    "        #Set Alpha value\n",
    "        alpha = float(Eaph)\n",
    "        #Set Epsilon value\n",
    "        epsilon = float(Eeps)\n",
    "\n",
    "        #Call to the main algorythm\n",
    "        Qlearn.train2(q, Qlearn.states, Qlearn.actions,episodes,initialState,goal_state, gamma, r, alpha, epsilon)\n",
    "        #Normalize the algorythm output\n",
    "        q = Qlearn.normalize(q, Qlearn.states, Qlearn.actions)\n",
    "        #Console print for result q\n",
    "        print(Qlearn.render(q, Qlearn.states, Qlearn.actions))\n",
    "        #Create result window\n",
    "        res = Tk()\n",
    "        res.title(\"Q-Learning\")\n",
    "        res.geometry(\"800x400\")\n",
    "        Lres = Label(res, text = Qlearn.render(q, Qlearn.states, Qlearn.actions))\n",
    "        Lres.pack()\n",
    "        Lpath = Label(res, text = \"Camino más corto: \"+  str(Qlearn.shortestpath(initialState, goal_state, q)))\n",
    "        Lpath.pack()\n",
    "\n",
    "    def fase3(Eep, Ego, Ega,Eis,Ech,Eer,Ebo, win2):\n",
    "        #Set Initial State\n",
    "        initialState = int(Eis)\n",
    "        #Set Checkpoints\n",
    "        if Ech != \"\":\n",
    "            checkpoints = Qlearn.stringToArrayInt(Ech,win2)\n",
    "            print(\"Traps: \" + str(checkpoints))\n",
    "            if checkpoints == False:\n",
    "                return\n",
    "        else:\n",
    "            checkpoints = \"\"\n",
    "        #Set Goal State\n",
    "        goal_state = int(Ego)\n",
    "        #Generate graph for creating the reward matrix\n",
    "        neighbors = Qlearn.find_neighbours(Qlearn.m)\n",
    "        if neighbors == False:\n",
    "            return\n",
    "        #Set Restrictions\n",
    "        if Eer != \"\":\n",
    "            restrictions = Qlearn.applyRestrictions(neighbors, str(Eer),win2)\n",
    "            print(\"Neighbors: \" + str(restrictions))\n",
    "            if restrictions == False:\n",
    "                return\n",
    "        #Boost Square\n",
    "        if Ebo != \"\":\n",
    "            boost = Qlearn.stringToArrayInt(Ebo,win2)\n",
    "            print(\"Boosts: \" + str(boost))\n",
    "            if boost == False:\n",
    "                return\n",
    "        else:\n",
    "            boost = \"\"\n",
    "        #Set the Reward and Q matrix dimensions\n",
    "        Qlearn.states = Qlearn.states * Qlearn.actions\n",
    "        Qlearn.actions = Qlearn.states\n",
    "        #Create Q Matrix with all 0\n",
    "        q = np.zeros((Qlearn.states, Qlearn.actions))\n",
    "        #Create the Reward matrix for the Entry Matrix\n",
    "        r= Qlearn.reward2(goal_state, Qlearn.states, Qlearn.actions, restrictions,boost, checkpoints)\n",
    "        #Set Episodes\n",
    "        episodes = int(Eep)\n",
    "        #Set Gamma\n",
    "        gamma = float(Ega)\n",
    "        #Destroy Matrix input window\n",
    "        win2.destroy()\n",
    "        #Call to Main Algorythm\n",
    "        Qlearn.train(q, Qlearn.states, Qlearn.actions,episodes,initialState,goal_state, gamma, r)\n",
    "        #Normalize Q output\n",
    "        q = Qlearn.normalize(q, Qlearn.states, Qlearn.actions)\n",
    "        #Print on console the result\n",
    "        print(Qlearn.render(q, Qlearn.states, Qlearn.actions))\n",
    "        #Create Result Window\n",
    "        res = Tk()\n",
    "        res.title(\"Q-Learning\")\n",
    "        res.geometry(\"800x400\")\n",
    "        Lres = Label(res, text = Qlearn.render(q, Qlearn.states, Qlearn.actions))\n",
    "        Lres.pack()\n",
    "        Lpath = Label(res, text = \"Camino más corto: \"+  str(Qlearn.shortestpath(initialState, goal_state, q)))\n",
    "        Lpath.pack()\n",
    "\n",
    "    def fase32(Eep, Ego, Ega,Eis,Ech,Eer,Ebo,Eaph,Eeps, win2):\n",
    "        #Set Initial State\n",
    "        initialState = int(Eis)\n",
    "        #Set Checkpoints\n",
    "        if Ech != \"\":\n",
    "            checkpoints = Qlearn.stringToArrayInt(Ech,win2)\n",
    "            print(\"Traps: \" + str(checkpoints))\n",
    "            if checkpoints == False:\n",
    "                return\n",
    "        else:\n",
    "            checkpoints = \"\"\n",
    "        #Set Goal State\n",
    "        goal_state = int(Ego)\n",
    "        #Generate graph for creating the reward matrix\n",
    "        neighbors = Qlearn.find_neighbours(Qlearn.m)\n",
    "        #Set Restrictions\n",
    "        if Eer != \"\":\n",
    "            restrictions = Qlearn.applyRestrictions(neighbors, str(Eer),win2)\n",
    "            print(\"Neighbors: \" + str(restrictions))\n",
    "            if restrictions == False:\n",
    "                return\n",
    "        #Boost Square\n",
    "        if Ebo != \"\":\n",
    "            boost = Qlearn.stringToArrayInt(Ebo,win2)\n",
    "            print(\"Boosts: \" + str(boost))\n",
    "            if boost == False:\n",
    "                return\n",
    "        else:\n",
    "            boost = \"\"\n",
    "        #Set the Reward and Q matrix dimensions\n",
    "        Qlearn.states = Qlearn.states * Qlearn.actions\n",
    "        Qlearn.actions = Qlearn.states\n",
    "        #Create Q Matrix with all 0\n",
    "        q = np.zeros((Qlearn.states, Qlearn.actions))\n",
    "        #Create the Reward matrix for the Entry Matrix\n",
    "        r= Qlearn.reward2(goal_state, Qlearn.states, Qlearn.actions, restrictions,boost, checkpoints)\n",
    "        #Set Episodes\n",
    "        episodes = int(Eep)\n",
    "        #Set Gamma\n",
    "        gamma = float(Ega)\n",
    "        #Set Alpha value\n",
    "        alpha = float(Eaph)\n",
    "        #Set Epsilon value\n",
    "        epsilon = float(Eeps)\n",
    "        #Destroy Matrix input window\n",
    "        win2.destroy()\n",
    "        #Call to Main Algorythm\n",
    "        Qlearn.train2(q, Qlearn.states, Qlearn.actions,episodes,initialState,goal_state, gamma, r, alpha, epsilon)\n",
    "        #Normalize Q output\n",
    "        q = Qlearn.normalize(q, Qlearn.states, Qlearn.actions)\n",
    "        #Print on console the result\n",
    "        print(Qlearn.render(q, Qlearn.states, Qlearn.actions))\n",
    "        print(Qlearn.shortestpath(initialState, goal_state, q))\n",
    "        #Create Result Window\n",
    "        res = Tk()\n",
    "        res.title(\"Q-Learning\")\n",
    "        res.geometry(\"800x400\")\n",
    "        Lres = Label(res, text = Qlearn.render(q, Qlearn.states, Qlearn.actions))\n",
    "        Lres.pack()\n",
    "        Lpath = Label(res, text = \"Camino más corto: \"+  str(Qlearn.shortestpath(initialState, goal_state, q)))\n",
    "        Lpath.pack()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Main App Front End"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Qlearn(Qlearn): \n",
    "    #Main App FrontEnd\n",
    "\n",
    "    def pargui(entrymatrix,previouswindow):\n",
    "        #Set the gameboard\n",
    "        Qlearn.m = entrymatrix\n",
    "        #Destroy the matrix input window\n",
    "        previouswindow.destroy()\n",
    "        #Param Window Generation\n",
    "        win2 = Tk()\n",
    "        vcmd=(win2.register(Qlearn.validateint), \"%P\")\n",
    "        vcmdfloat =(win2.register(Qlearn.validatefloat), \"%P\")\n",
    "        win2.title(\"Q-Learning parameters\")\n",
    "        win2.geometry(\"1000x500\")\n",
    "        #Episodes Entry\n",
    "        Lep = Label(win2, text=\"Episodes\")\n",
    "        Lep.pack(side=LEFT)\n",
    "        Lep.place(x=20, y=10)\n",
    "        Eep = Entry(win2, bd=5,validate=\"key\",validatecommand=vcmd)\n",
    "        Eep.pack(side=RIGHT)\n",
    "        Eep.place(x=150, y=10)\n",
    "        #Episodes Explanation\n",
    "        LepExp = Label(win2, text=\"Number of Episodes to test\")\n",
    "        LepExp.pack()\n",
    "        LepExp.place(x = 300, y =10)\n",
    "        #Initial State\n",
    "        Lis = Label(win2, text=\"Initial State\")\n",
    "        Lis.pack(side=LEFT)\n",
    "        Lis.place(x=20, y=40)\n",
    "        Eis = Entry(win2, bd=5,validate=\"key\",validatecommand=vcmd)\n",
    "        Eis.pack(side=RIGHT)\n",
    "        Eis.place(x=150, y=40)\n",
    "        #Initial State Explanation\n",
    "        LisExp = Label(win2, text=\"Initial State of the Agent\")\n",
    "        LisExp.pack()\n",
    "        LisExp.place(x = 300, y =40)\n",
    "        #Goal State\n",
    "        Lgo = Label(win2, text=\"Goal State\")\n",
    "        Lgo.pack(side=LEFT)\n",
    "        Lgo.place(x=20, y=70)\n",
    "        Ego = Entry(win2, bd=5,validate=\"key\",validatecommand=vcmd)\n",
    "        Ego.pack(side=RIGHT)\n",
    "        Ego.place(x=150, y=70)\n",
    "        #Goal State Explanation\n",
    "        LgoExp = Label(win2, text=\"Final State of the Agent\")\n",
    "        LgoExp.pack()\n",
    "        LgoExp.place(x = 300, y =70)\n",
    "        #Gamma\n",
    "        Lga = Label(win2, text=\"Gamma\")\n",
    "        Lga.pack(side=LEFT)\n",
    "        Lga.place(x=20, y=100)\n",
    "        Ega = Entry(win2, bd=5,validate=\"key\",validatecommand=vcmdfloat)\n",
    "        Ega.pack(side=RIGHT)\n",
    "        Ega.place(x=150, y=100)\n",
    "        #Goal State Explanation\n",
    "        LgaExp = Label(win2, text=\"Future reward looking ratio, the greater it is more will evaluate the potential rewards of a path\")\n",
    "        LgaExp.pack()\n",
    "        LgaExp.place(x = 300, y =100)\n",
    "        #Button\n",
    "        B = Button(win2, text = \"Q-Learning\", command= lambda: Qlearn.passToFase1(Eep, Ego, Ega,Eis.get(),win2))\n",
    "        B.pack()\n",
    "        B.place(x=100, y = 130)\n",
    "        #Fase 1 Explanation\n",
    "        Lf1 = Label(win2, text=\"All of the fields above are REQUIRED\")\n",
    "        Lf1.pack()\n",
    "        Lf1.place(x=300, y = 130)\n",
    "        \n",
    "        #Epsilon\n",
    "        Leps = Label(win2, text=\"Epsilon\")\n",
    "        Leps.pack(side=LEFT)\n",
    "        Leps.place(x=20, y = 160)\n",
    "        Eeps = Entry(win2, bd=5,validate=\"key\",validatecommand=vcmdfloat)\n",
    "        Eeps.pack(side = RIGHT)\n",
    "        Eeps.place(x=150, y = 160)\n",
    "        #Epsilon Explanation\n",
    "        LepsExp = Label(win2, text=\"Choice Politics Cte, range [0,1]\")\n",
    "        LepsExp.pack()\n",
    "        LepsExp.place(x = 300, y =160)\n",
    "        #Alpha\n",
    "        Laph = Label(win2, text=\"Alpha\")\n",
    "        Laph.pack(side = LEFT)\n",
    "        Laph.place(x=20, y=190)\n",
    "        Eaph = Entry(win2, bd=5,validate=\"key\",validatecommand=vcmdfloat)\n",
    "        Eaph.pack(side= LEFT)\n",
    "        Eaph.place(x=150, y=190)\n",
    "        #Alpha Explanation\n",
    "        LahpExp = Label(win2, text=\"Epsilon multiplier, range [0,1]\")\n",
    "        LahpExp.pack()\n",
    "        LahpExp.place(x = 300, y= 190)\n",
    "        #Button fase 2\n",
    "        B2 = Button(win2, text=\"Q-Learning with Exploration Rate\", command = lambda: Qlearn.passToFase2(Eep.get(), Ego.get(), Ega.get(), Eeps.get(), Eaph.get(),Eis.get(), win2))\n",
    "        B2.pack()\n",
    "        B2.place(x=100, y = 210)\n",
    "        #Fase 2 Explanation\n",
    "        Lf2 = Label(win2, text=\"All of the fields above are REQUIRED\")\n",
    "        Lf2.pack()\n",
    "        Lf2.place(x=300, y = 210)\n",
    "\n",
    "        #Pifalls States\n",
    "        Lch = Label(win2, text=\"Traps\")\n",
    "        Lch.pack(side=LEFT)\n",
    "        Lch.place(x = 20, y = 240)\n",
    "        Ech = Entry(win2, bd=5)\n",
    "        Ech.pack(side=RIGHT)\n",
    "        Ech.place(x = 150 , y = 240)\n",
    "        #Pitfall Explanation\n",
    "        LchExp = Label(win2, text=\"Trap Squares (-50 on the reward matrix), format example: 1,2,3\")\n",
    "        LchExp.pack()\n",
    "        LchExp.place(x = 300, y =240)\n",
    "        #Restrictions\n",
    "        Lre = Label(win2, text=\"Restrictions\")\n",
    "        Lre.pack(side=LEFT)\n",
    "        Lre.place(x=20, y=270)\n",
    "        Ere = Entry(win2, bd=5)\n",
    "        Ere.pack(side=RIGHT)\n",
    "        Ere.place(x=150, y=270)\n",
    "        #Forbidden Moves Explanation\n",
    "        LreExp = Label(win2, text=\"Forbidden Moves, format example: 0-1;1-3;1-4\")\n",
    "        LreExp.pack()\n",
    "        LreExp.place(x = 300, y =270)\n",
    "        #Boost Square\n",
    "        Lbo = Label(win2, text=\"Boost Square\")\n",
    "        Lbo.pack(side=LEFT)\n",
    "        Lbo.place(x=20, y=300)\n",
    "        Ebo = Entry(win2, bd=5)\n",
    "        Ebo.pack(side=RIGHT)\n",
    "        Ebo.place(x=150, y=300)\n",
    "        #Boost Explanation\n",
    "        LreExp = Label(win2, text=\"Boost Square (+10 on the reward matrix), format example: 0,1,2\")\n",
    "        LreExp.pack()\n",
    "        LreExp.place(x = 300, y =300)\n",
    "\n",
    "        B3 = Button(win2, text=\"Q-Learning with restrictions\", command = lambda:Qlearn.passToFase3(Eep.get(), Ego.get(), Ega.get(), Eis.get(), Ech.get(), Ere.get(),Ebo.get(), win2))\n",
    "        B3.pack()\n",
    "        B3.place(x=100, y = 330)\n",
    "        #Fase 3 1 Explanation\n",
    "        Lf3 = Label(win2, text=\"All of the fields above are REQUIRED except Epsilon and Alpha. Traps, Restrictions and Boosts are Optional\")\n",
    "        Lf3.pack()\n",
    "        Lf3.place(x=300, y = 330)\n",
    "\n",
    "        B4 = Button(win2, text=\"Q-Learning with restrictions and Exploration Rate\", command = lambda:Qlearn.passToFase32(Eep.get(), Ego.get(), Ega.get(), Eis.get(), Ech.get(), Ere.get(),Ebo.get(), Eaph.get(), Eeps.get(), win2))\n",
    "        B4.pack()\n",
    "        B4.place(x=100, y = 360)\n",
    "        #Fase 3 2 Explanation\n",
    "        Lf3 = Label(win2, text=\"All of the fields above are REQUIRED. Traps, Restrictions and Boosts are Optional\")\n",
    "        Lf3.pack()\n",
    "        Lf3.place(x=400, y = 360)\n",
    "        win2.mainloop()\n",
    "\n",
    "    def magui(Eest, Eact,win):\n",
    "        #Rows\n",
    "        Qlearn.states = int(Eest.get())\n",
    "        #Columns\n",
    "        Qlearn.actions= int(Eact.get())\n",
    "        #Destroy previous window\n",
    "        win.destroy()\n",
    "        #Create Matrix Input Window\n",
    "        ven = Tk()\n",
    "        ven.title(\"Matrix Input\")\n",
    "        #Matrix Input Frame\n",
    "        Framegeneration(ven,Qlearn.states,Qlearn.actions).pack(side=\"top\", fill=\"both\", expand=True)\n",
    "        \n",
    "    def startgui():\n",
    "    #Dimension Window generation\n",
    "        win = Tk()\n",
    "        #Validation command for int\n",
    "        vcmd=(win.register(Qlearn.validateint), \"%P\")\n",
    "        win.title(\"Q-Learning\")\n",
    "        win.geometry(\"300x300\")\n",
    "        #States Entry\n",
    "        Lest = Label(win, text=\"Rows\")\n",
    "        Lest.pack(side=LEFT)\n",
    "        Lest.place(x=20, y=10)\n",
    "        Eest = Entry(win, bd=5, validate=\"key\", validatecommand=vcmd)\n",
    "        Eest.pack(side= RIGHT)\n",
    "        Eest.place(x=150, y = 10)\n",
    "        #Actions Entry\n",
    "        Lact = Label(win, text=\"Columns\")\n",
    "        Lact.pack(side=LEFT)\n",
    "        Lact.place(x=20, y=40)\n",
    "        Eact = Entry(win, bd=5, validate=\"key\", validatecommand=vcmd)\n",
    "        Eact.pack(side = RIGHT)\n",
    "        Eact.place(x=150, y =40)\n",
    "        #Button\n",
    "        B = Button(win, text = \"Q-Learning\", command= lambda: Qlearn.magui(Eest, Eact,win))\n",
    "        B.pack()\n",
    "        B.place(x=100, y = 70)\n",
    "        win.mainloop()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initial States"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Qlearn(Qlearn):\n",
    "    #Initial State\n",
    "    #State, Actions and the Gameboard (m) are stored as a Class Variable\n",
    "    states = 0\n",
    "    actions = 0\n",
    "    m = []\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Application Start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Try again next time\n"
     ]
    }
   ],
   "source": [
    "#Application Start\n",
    "#Start the system with the GUI\n",
    "Qlearn.startgui()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
